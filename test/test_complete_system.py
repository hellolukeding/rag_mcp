#!/usr/bin/env python3
"""
å®Œæ•´ç³»ç»Ÿæµ‹è¯•
æµ‹è¯•å‘é‡åŒ–æœåŠ¡çš„å®Œæ•´æµç¨‹ï¼šæ–‡ä»¶ä¸Šä¼  -> å‘é‡åŒ– -> æŸ¥è¯¢ -> åˆ é™¤
"""

import asyncio
import os
import shutil
import tempfile
from pathlib import Path

from core.vectorize import get_vectorize_service
from database.models import DatabaseManager
from utils.logger import setup_logger

# è®¾ç½®æ—¥å¿—
logger = setup_logger(__name__)


async def test_complete_system():
    """æµ‹è¯•å®Œæ•´ç³»ç»Ÿæµç¨‹"""
    logger.info("ğŸš€ å¼€å§‹å®Œæ•´ç³»ç»Ÿæµ‹è¯•...")

    # åˆå§‹åŒ–æ•°æ®åº“
    db_manager = DatabaseManager()
    await db_manager.init_database()
    logger.info("âœ… æ•°æ®åº“åˆå§‹åŒ–å®Œæˆ")

    # è·å–å‘é‡åŒ–æœåŠ¡
    vectorize_service = get_vectorize_service()
    vectorize_service.start()
    logger.info("âœ… å‘é‡åŒ–æœåŠ¡å¯åŠ¨å®Œæˆ")

    try:
        # åˆ›å»ºæµ‹è¯•ç›®å½•
        test_dir = Path("test/temp_system_test")
        test_dir.mkdir(exist_ok=True)

        # åˆ›å»ºå¤šä¸ªæµ‹è¯•æ–‡ä»¶
        test_files = []
        test_contents = [
            "è¿™æ˜¯ç¬¬ä¸€ä¸ªæµ‹è¯•æ–‡æ¡£ï¼ŒåŒ…å«å…³äºäººå·¥æ™ºèƒ½çš„å†…å®¹ã€‚",
            "è¿™æ˜¯ç¬¬äºŒä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œè®¨è®ºæœºå™¨å­¦ä¹ å’Œæ·±åº¦å­¦ä¹ æŠ€æœ¯ã€‚",
            "è¿™æ˜¯ç¬¬ä¸‰ä¸ªæµ‹è¯•æ–‡æ¡£ï¼Œä»‹ç»è‡ªç„¶è¯­è¨€å¤„ç†çš„åº”ç”¨ã€‚"
        ]

        logger.info("ğŸ“ åˆ›å»ºæµ‹è¯•æ–‡ä»¶...")
        for i, content in enumerate(test_contents, 1):
            file_path = test_dir / f"test_doc_{i}.txt"
            file_path.write_text(content, encoding='utf-8')
            test_files.append(file_path)
            logger.info(f"âœ… åˆ›å»ºæµ‹è¯•æ–‡ä»¶: {file_path.name}")

        # 1. æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å’Œè®°å½•åˆ›å»º
        logger.info("\nğŸ”„ æ­¥éª¤1: æµ‹è¯•æ–‡ä»¶ä¸Šä¼ å’Œè®°å½•åˆ›å»º...")
        file_records = []
        for file_path in test_files:
            # æ¨¡æ‹Ÿæ–‡ä»¶ä¸Šä¼ 
            file_record = await db_manager.create_file_record(
                file_id=f"test_{file_path.stem}",
                original_name=file_path.name,
                file_name=file_path.name,
                file_path=str(file_path),
                file_type="text/plain",
                file_size=file_path.stat().st_size
            )
            file_records.append(file_record)
            logger.info(f"âœ… åˆ›å»ºæ–‡ä»¶è®°å½•: {file_record.file_id}")

        # 2. æµ‹è¯•å‘é‡åŒ–
        logger.info("\nğŸ”„ æ­¥éª¤2: æµ‹è¯•æ–‡ä»¶å‘é‡åŒ–...")
        tasks = []
        for file_record in file_records:
            task = vectorize_service.add_task(file_record)
            tasks.append(task)
            logger.info(f"âœ… æ·»åŠ å‘é‡åŒ–ä»»åŠ¡: {task.task_id}")

        # ç­‰å¾…æ‰€æœ‰å‘é‡åŒ–ä»»åŠ¡å®Œæˆ
        logger.info("â³ ç­‰å¾…å‘é‡åŒ–å®Œæˆ...")
        completed_count = 0
        while completed_count < len(tasks):
            await asyncio.sleep(0.5)
            completed_count = sum(
                1 for task in tasks if task.status == "completed")
            progress = (completed_count / len(tasks)) * 100
            logger.info(
                f"ğŸ“Š å‘é‡åŒ–è¿›åº¦: {progress:.1f}% ({completed_count}/{len(tasks)})")

        logger.info("âœ… æ‰€æœ‰æ–‡ä»¶å‘é‡åŒ–å®Œæˆ!")

        # 3. éªŒè¯å‘é‡åŒ–ç»“æœ
        logger.info("\nğŸ”„ æ­¥éª¤3: éªŒè¯å‘é‡åŒ–ç»“æœ...")
        for file_record in file_records:
            # æ£€æŸ¥æ–‡ä»¶çŠ¶æ€
            updated_record = await db_manager.get_file_by_id(file_record.file_id)
            if updated_record.vectorized == 'completed':
                logger.info(f"âœ… æ–‡ä»¶ {updated_record.file_name} å‘é‡åŒ–æˆåŠŸ")
            else:
                logger.error(f"âŒ æ–‡ä»¶ {updated_record.file_name} å‘é‡åŒ–å¤±è´¥")

            # æ£€æŸ¥æ–‡æ¡£è®°å½•
            documents = await db_manager.get_documents_by_file_id(file_record.file_id)
            if documents:
                logger.info(f"ğŸ“‘ æ–‡æ¡£è®°å½•å­˜åœ¨: {len(documents)} ä¸ª")

                # æ£€æŸ¥æ–‡æ¡£å—
                for doc in documents:
                    chunks = await db_manager.get_chunks_by_document_id(doc.id)
                    logger.info(f"ğŸ§© æ–‡æ¡£ {doc.id} æœ‰ {len(chunks)} ä¸ªå‘é‡å—")
            else:
                logger.error(f"âŒ æœªæ‰¾åˆ°æ–‡æ¡£è®°å½•")

        # 4. æµ‹è¯•æ•°æ®ç»Ÿè®¡
        logger.info("\nğŸ”„ æ­¥éª¤4: æ£€æŸ¥æ•°æ®ç»Ÿè®¡...")

        # ç»Ÿè®¡æ•°æ®
        async with db_manager.get_connection() as db:
            # æ–‡ä»¶ç»Ÿè®¡
            async with db.execute("SELECT COUNT(*) FROM files") as cursor:
                files_count = await cursor.fetchone()
                logger.info(f"ğŸ“„ æ€»æ–‡ä»¶æ•°: {files_count[0]}")

            # æ–‡æ¡£ç»Ÿè®¡
            async with db.execute("SELECT COUNT(*) FROM documents") as cursor:
                docs_count = await cursor.fetchone()
                logger.info(f"ğŸ“‘ æ€»æ–‡æ¡£æ•°: {docs_count[0]}")

            # å‘é‡å—ç»Ÿè®¡
            async with db.execute("SELECT COUNT(*) FROM document_chunks") as cursor:
                chunks_count = await cursor.fetchone()
                logger.info(f"ğŸ§© æ€»å‘é‡å—æ•°: {chunks_count[0]}")

            # å·²å‘é‡åŒ–æ–‡ä»¶ç»Ÿè®¡
            async with db.execute("SELECT COUNT(*) FROM files WHERE vectorized = 'completed'") as cursor:
                vectorized_count = await cursor.fetchone()
                logger.info(f"âœ… å·²å‘é‡åŒ–æ–‡ä»¶æ•°: {vectorized_count[0]}")

        # 5. æµ‹è¯•åˆ é™¤åŠŸèƒ½
        logger.info("\nğŸ”„ æ­¥éª¤5: æµ‹è¯•æ–‡ä»¶åˆ é™¤åŠŸèƒ½...")

        # åˆ é™¤ç¬¬ä¸€ä¸ªæ–‡ä»¶
        test_file_record = file_records[0]
        logger.info(f"ğŸ—‘ï¸ åˆ é™¤æ–‡ä»¶: {test_file_record.file_name}")

        success = await db_manager.delete_file_and_documents(test_file_record.file_id)
        if success:
            logger.info("âœ… æ–‡ä»¶åˆ é™¤æˆåŠŸ")

            # éªŒè¯åˆ é™¤ç»“æœ
            deleted_file = await db_manager.get_file_by_id(test_file_record.file_id)
            if deleted_file is None:
                logger.info("âœ… æ–‡ä»¶è®°å½•å·²åˆ é™¤")
            else:
                logger.error("âŒ æ–‡ä»¶è®°å½•æœªåˆ é™¤")

            # æ£€æŸ¥ç›¸å…³æ–‡æ¡£æ˜¯å¦åˆ é™¤
            documents = await db_manager.get_documents_by_file_id(test_file_record.file_id)
            if not documents:
                logger.info("âœ… ç›¸å…³æ–‡æ¡£å·²åˆ é™¤")
            else:
                logger.error(f"âŒ ä»æœ‰ {len(documents)} ä¸ªæ–‡æ¡£æœªåˆ é™¤")
        else:
            logger.error("âŒ æ–‡ä»¶åˆ é™¤å¤±è´¥")

        # 6. æœ€ç»ˆç»Ÿè®¡
        logger.info("\nğŸ”„ æ­¥éª¤6: æœ€ç»ˆæ•°æ®ç»Ÿè®¡...")
        async with db_manager.get_connection() as db:
            async with db.execute("SELECT COUNT(*) FROM files") as cursor:
                final_files_count = await cursor.fetchone()
                logger.info(f"ğŸ“„ æœ€ç»ˆæ–‡ä»¶æ•°: {final_files_count[0]}")

            async with db.execute("SELECT COUNT(*) FROM documents") as cursor:
                final_docs_count = await cursor.fetchone()
                logger.info(f"ğŸ“‘ æœ€ç»ˆæ–‡æ¡£æ•°: {final_docs_count[0]}")

            async with db.execute("SELECT COUNT(*) FROM document_chunks") as cursor:
                final_chunks_count = await cursor.fetchone()
                logger.info(f"ğŸ§© æœ€ç»ˆå‘é‡å—æ•°: {final_chunks_count[0]}")

        logger.info("ğŸ‰ å®Œæ•´ç³»ç»Ÿæµ‹è¯•å®Œæˆ!")

    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
        raise
    finally:
        # åœæ­¢å‘é‡åŒ–æœåŠ¡
        vectorize_service.stop()
        logger.info("ğŸ›‘ å‘é‡åŒ–æœåŠ¡å·²åœæ­¢")

        # æ¸…ç†æµ‹è¯•æ–‡ä»¶
        if test_dir.exists():
            shutil.rmtree(test_dir)
            logger.info("ğŸ—‘ï¸ æ¸…ç†æµ‹è¯•æ–‡ä»¶")


async def main():
    """ä¸»å‡½æ•°"""
    try:
        await test_complete_system()
        logger.info("âœ… æµ‹è¯•æˆåŠŸå®Œæˆ")
    except Exception as e:
        logger.error(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        raise


if __name__ == "__main__":
    asyncio.run(main())
